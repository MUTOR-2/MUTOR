---
###
# unit information: 
# Music and Language 
###
title: Title of the unit
number: 0
short_description: short description of the unit
summary: brief summary of the unit
authors: 
 - author 1
topics: [topic 1, topic 2]
test_questions:
 - question 1
 - question 2

###
# page layout:
# don't change
###
layout: unit
citations: ""
mathjax: true
---

{% include unit_preamble.md %}

# Music and Language 
Music and language are both fundamental to human experience. The two domains, while apparently different, rely on several notable similarities: both exhibit syntactic structure, and both rely on sensory, cognitive, and vocal-motor apparatus of the central nervous system. 

# Evidence for shared processing between music and language 
In the field of neuropsychology, the double dissociations between amusia and aphasia have been used to claim that music and language do not share common neural substrates. On the other hand, neuroimaging evidence (from functional and structural Magnetic Resonance Imaging, as well as electrophysiological recordings) show similar activations between musical and linguistic stimuli, which stands as evidence in support of the overlap between music and language. Patel (2008) points out that several aphasics without amusia are professional musicians whose data cannot be generalized to ordinary individuals. 

One methodological issue in characterizing musicality is that tests used to verify musicality and/or musical competences are not very sensitive and well refined. 
An explanatory theory to account for discrepancies between neuroimaging and neuropsychological data is needed.

# Phonology
Phonology refers to the sounds of speech and language. Abundant evidence suggests that phonological processing may be influenced by musical training. For example, phonological ability is a second language is better in people who have musical training (Slevc et al, 2006). Children who are better able to perceive and produce pitches also perform better at tasks of phonological awareness, which requires manipulating speech sounds in the mind (Loui et al, 2011). 

# Syntax
Syntax refers to the ordering of entities to form coherent strings. In language, this refers to the order of words and phrases to form sentences. In music, syntax refers to the ordering of pitches, chords, and rhythms to form melodies and harmonies. The nature of the relationship between syntactic structure in language and music, and their underlying neural substrates, is a topic of intense interest to the cognitive and brain sciences community.

The Shared Syntactic Integration Resource Hypothesis (SSIRH, Patel, 2008) is an influential theoretical account of similarities and differences between cognitive processing for music and language. The SSIRH posits that neural resources for music and language overlap at the level of the syntax; in other words, processing of music and language should interact at the syntactic level, but not at other levels such as semantics or acoustic or phonemic structure. Support for the SSIRH comes from a variety of behavioral and neural studies (see Patel, 2008 for a review). 

# Semantics
Semantics refers to the study of meaning. In the study of semantics in language, influential results have come from studies that involve reading sentences with expected words being occasionally replaced by ssemantically unexpected words. Consider this example:

I take my coffee with cream and socks.

Since the word "socks" is unexpected in meaning, it is incongruous with the semantic expectation in most English speakers. By comparing brain responses for this semantically incongruous sentence against brain responses for expected sentences, i.e. "I take my coffee with cream and sugar." we can observe how the brain responsds to semantics. From electrophysiology studies, the onset of the word "wocks" is known to elicit a negative waveform around 400 milliseconds after the onset of the unexpected word. This is known as the N400 effect. 

# Prosody
Prosody refers to the sounds of speech: the stress patterns, the pitch changes, and the rhythm within speech. These are parts of speech that are not usually written down. The prosody of speech can be appreciated when listening to low-pass filtered speech sounds, or sine wave speech [http://www.lifesci.sussex.ac.uk/home/Chris_Darwin/SWS/]. These speech samples, while not always comprehensible, bring out the prosodic elements (pitch and pitch changes, rhythm and timing content) of speech.

Another aspect of speech in which prosody comes across clearly is in accents. Stress patterns and pitch patterns differ clearly between different accents, as can be seen here: [https://m.youtube.com/watch?v=3UgpfSp2t6k]. 

The rhythmic content of speech can be captured using the normalized Pairwise Variability Index (nPVI): the ratio between successive durations of syllables within (formula given here) [https://www.researchgate.net/profile/Hua_Lin13/publication/228773149/figure/fig3/AS:667829170892804@1536234290353/nPVI-formula_Q320.jpg]. 

As in the examples, the speaker is able to manipulate her speech sound duration, the pattern of duration of different syllables, and the direction of her pitches is effective in conveying nationality or regionality, or where the speaker comes from. 

In music, composers and performers may also employ certain compositional devices to convey a national or regional accent. Here is an examples of very "French-sounding" music: Debussy La Mer https://m.youtube.com/watch?v=hlR9rDJMEiQ  

Here is an example of very "English-sounding" music:
Elgar Violin Concerto https://m.youtube.com/watch?v=9rCVW_4qwRY  

nPVI of English speech is higher than for French speech, because of the different syllabic structures of French and English speech. Daniele and Patel (2003) extended this result to music, and showed that the nPVI of music by English composers is higher than music by French composers.

Lolli et al (2015) showed a correlation between pitch perception and emotional content in speech: people who were better at discriminating small differences in pitch (i.e. better pitch discrimination skills) were also better at identifying the emotional content from speech, i.e. telling between "happy-sounding" or "sad-sounding" speech. This relationship was strongest when the speech was low-pass filtered. This finding has several implications: 1) the prosodic content is part of emotional content of speech, 2) this emotional content is contained in low-frequency information within speech, and 3) people with musical training or musical ability may be more sensitive to emotional content. 

{% include unit_postamble.md %}
