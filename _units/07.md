---
###
# unit information: 
# Rhythm
###
title: Rhythm
number: 0
short_description: short description of the unit
summary: brief summary of the unit
authors: 
 - Psyche Loui
 - John MacCallum
topics: [topic 1, topic 2]
test_questions:
 - question 1
 - question 2

###
# page layout:
# don't change
###
layout: unit
citations: ""
mathjax: true
---

{% include unit_preamble.md %}

# Rhythm

## Introduction

TactuS
TatuM
AccentsAndEventStreamVectors
MicrotimingAndExpressiveTiming
TimeMaps
ScoreTimeVsPerformanceTime
RhythmoGram
ModelsOfRhythmPerception
IntervalTiming
CovarianceModel
CoupledOscillator
SumMary
ReferenceS
LinksnDownloads
QuizItems

While pitch has received the most attention in perception research, whereas most of music theory consists of putting pitches together to form harmonies, relatively little of common-practice Western musical theory addresses rhythm. Here we define rhythm as the pattern of temporal durations, especially in music. In this unit we will examine the perception and cognition of rhythm. We will begin with the basic observations, e.g. what the optimal range of rhythm and meter are, and how rhythmic subdivisions and expressive timing occur. Then we will discuss the various representations of rhythm, somewhat analogous to visual representations of pitch covered in Unit 5. Finally we will talk about models of rhythm perception and production, and bring in existing literature in an attempt to explain the temporal regularities and deviations that give rise to the human sense of rhythm.

## Perceptual Onset Vs Temporal Envelope
It is important to make a distinction between the temporal envelope, the attack portion of which we usually consider to be the beginning of the sound, and the perceptual onset of a sound. In the case of a percussive sound made by a bell or a piano for example, the attack is nearly instantaneous, while other instruments like violins and clarinets have a variety of attacks at their disposal which can make it difficult pinpoint the onset of their sound. Attacks that last less than 30ms tend to sound percussive, while attacks that are longer than 30ms are more like those of a bowed instrument. In the demo called attack-sync, you will hear two sounds, one with a long attack and one with a short percussive attack. When you start the patch, the two sounds will begin together, although, as you will notice, the one with the longer attack will appear to start later. See if you can adjust the start times such that they appear to start isochronously.

attack sync-ss

## Subdivision
The optimal range of tempo encoding occurs between 300ms and 1500ms. This is known as the zone of temporal integration, or the tactus. What happens when rhythmic durations are way above tactus? It turns out we subdivide. Subdivision refers to the breaking down of large units into usually even-sized smaller units. In the rhythmic sense the brain is performing chunking, and also building a hierarchical structure of rhythm. As in the frequency dimension for pitch and harmony (see units 5 and 8), small-integer ratios in time play important roles in rhythmic perception. Most rhythmic subdivisions consist of 2:1 ratios, especially in Western music. Rhythms in music of other cultures, e.g. clavé rhythmic patterns in Afro-Cuban and Brazilian rhythms, employ more complex rhythmic patterns, but which can also be broken down into chunks of 2:1 temporal ratios (see Toussaint).

1. it helps working memory by helping chunking

2. clave and bell maps. Toussaint papers - relate somehow.

3. Four dots getting divided into chunks of two - think Gestalt and make pictures.

## Tactus
Using the demo on synchronous clapping, we may observe that when asked to tap in sync to a rhythmic stimulus, people commonly produce isochronous taps at approximately 1-3 Hz (75-200 taps per minute), i.e. with intervals of approximately 300-800ms between each tap. When the musical stimulus to which we are synchronizing contains sound events below this rate, we tend to subdivide the time by tapping twice per sound event. When the sound to which we are synchronizing is above this rate, we tend to subdivide our tapping by tapping once for every two beats. This optimal rate of rhythmic tapping is called the tactus and is in the range of 300-800Hz. The fast end of this range, 300ms or 200 beats per minute, corresponds to the tempo marking of Presto. The slow end, 800ms or 75 beats per minute, corresponds to Adagio. Assuming that our tapping rate reflects our perception of the beat, we can say that the tactus is the optimal range of rhythmic function in music.
Several lines of research have addressed why the tactus seems to operate at this range. Some believe that the tactus aids memory: the tactus may reflect the perceptual system's ability to group distinct sound events together in memory, so that instead of perceiving individual sounds which decay rapidly in sensory memory, we could perceive groups of rhythmic events which hang together to form a Gestalt, or the perception of a whole. Paul Fraisse (from Clarke's chapter, in Deutsch 1999) believed that the tactus arises from the anatomical constraints of the body and motor functions, as our bodies (musculature, nervous systems, etc) prevents us from tapping at rates faster than 3Hz.

## Tatum
In a paper called A Novel Representation for Rhythmic Structure, Vijay Iyer, Jeff Bilmes, Matt Wright, and David Wessel developed a structure for describing rhythmic events called the tatum (a contraction of temporal atom as well as an homage to the great improvising pianist, Art Tatum). A tatum is a data structure that represents the smallest cognitively meaningful subdivision of the main beat and contains information about it's probability of occurence, pitch/timbre, accent, duration, and deviation. Musically speaking, a tatum might correspond to 16th notes or 32nd notes in conventional notation, and a measure of music is represented by a vector of tatums. A high probability of occurence means that the beat is likely to be played (after all, not every beat of a bar is played all the time), the accent determines how loudly the beat will be played (this allows for the beginning of the beat of a conventional meter to be accented, for example), the duration vector could either determine the duration of each beat or represent a range of durations from which a random value is chosen, and the deviation allows for the beats to occur slightly early or late and is meant to model the sort of microtiming that human performers inadvertantely add to their music and that computers lack.

## Accents and Event Stream Vectors
A piece of music is represented by an event-stream vector which contains all of the data for every tatum. An example of a vector representing a bar of 4/4 music might have a set of vectors as represented in table 1.

Beat	Probability	Accent	Duration	Deviation

1	1	1	1	.1

2	.5	.25	1	.1

3	1	.75	1	.1

4	.75	.5	1	.1

Table 1.

In 4/4, the first beat is accented the strongest, the third beat the next strongest, the fourth beat is accented slightly as an upbeat to the next bar, and the second beat is the weakest. This is reflected in the probablity of occurence and accent vectors in table 1. The duration in this case is set at 1 which would signify a full beat and the deviation is very low. Table 2 represents a series of tatums that would produce a waltz-like feel.

Beat	Probability	Accent	Duration	Deviation

1	1	1	1	0

2	.5	.25	.5	.1

3	.75	.75	.5	.1

Table 2.

In the above examples, only the main beats of the measures are represented, however, in practice the vectors would contain information about every rhythmic event down to the smallest subdivision.

## Microtiming and Expressive Timing
With respect to rhythm, performances of music are often considered cold and lacking in feeling. It is tempting to think that if the metronome marking is 60 BPM, that the 16th-notes should go by at precisely 250 ms. Human performances, however are filled with small deviations in timing, often called microtiming or expressive timing. These small deviations, while not always overtly noticible, contribute to that human feeling that computer performances lack. In the following two examples, you will hear a melody from Robert Schumann's Traumerei, the first played without expressive timing, while the second includes deviations in timing. 

mechanical timing

expressive timing

(source: Penel & Drake, 2000. Rhythm in music performance and perceived structure. In Desain & Windsor, Ed.)

Above, in the section on the TatuM, along with the parameters of probability of occurence, duration, etc., we also defined a deviation vector. This deviation in timing is meant to mimic the microtiming found in human performance.

## Time Maps

Time maps are visual representations of rhythm and meter. These are useful because they help us visualize, analyze, and understand the black box of each individual performance by each musician. 

Two kinds of time maps we introduce here are the rhythmogram and the maps of performance time as a function of score time. These two time maps are drastically different and are useful in different applications. The rhythmogram is a time map of event frequencies of notes, with enables the inference of the hierarchical rhythmic structure of a musical piece. The maps of score time versus performance time allow for a coherent representation of the entire performance of a piece; they effectively capture the characteristic expressive deviations of each performer. 

In addition to the rhythmogram and the score vs performance time map, various other rhythmic representations deserve mention. Desain and Honing describe a triangular space that represents all possible expressive deviations given a four-note rhythm.

(image source: Desain & Honing, 2003. Perception.)

Other statistical methods to analyze rhythm and expressive microtiming are also widely used. Benadon uses histograms, which are graphs plotting the occurrence frequencies of events, to diagram the beat-upbeat ratios (BURs) of the swing rhythm in jazz.

## Score Time Vs Performance Time
Given the discussion of microtiming above, it should be clear by now that there is not necessarily a 1:1 correlation between the printed score and what a performer plays. A simple score may only contain pitches, rhythms, some dynamics, and information about the tempo and meter. Although it may lack direction about how to vary the timing expressively, we are not necessarily meant to interpret that lack of direction as an indication that we should play the music as straight and precisely as possible. When directions are given, they are often vague at best, as in the term rubato. As we mentioned above, a computer simulation that lacks microtiming and is based only on what we might call score time tends to feel cold and mechanical. We can begin to understand what a performer brings to the piece with respect to expressive timing by plotting the score time against the performance time as in figure 1.

score time vs. performance time

Figure 1. Score time versus performance time.

By modelling the relationship between score and performance time, one can create a synthesizer that will play music with a more human feel (A Novel Representation for Rhythmic Structure).

## Rhythmogram
The rhythmogram is an effective way to visualize rhythmic structure in a musical score or performance. First used by Neil Todd 
(mentioned in E. Clarke in Deutsch, 1999), the rhythmogram is a graph of musical performances with time on the x axis and a low-frequency filterbank on the y axis.

To visualize the low-frequency filterbank, consider a set of very low-frequency bandpass filters arranged logarithmically. For instance, several filters along the filterbank could be: 0.125Hz, 0.25Hz, 0.5Hz, 1Hz, 2Hz, and 4Hz. The rhythmogram plots musical time on the x axis and these filters on the y axis.

The axes of the rhythmogram: frequency filterbank vs. time.

In this representation, notes which occur quickly, e.g. sixteenth notes, would show activity at the highest frequency filters along the filterbank.

Fast notes activate high-frequency filters along the filterbank.

Notes occurring at the next fastest level, i.e. eighth notes, would appear as activity at the next level of filters:
 
The next level of notes (quarter notes) would then appear energy at 1Hz filters, half notes at 0.5Hz filters, and finally, at the highest level, whole notes would appear at 0.25Hz filters. 

Taken together, we see an emergent tree-like diagram of rhythmic activity, with the different layers of the filterbank representing each level of rhythmic structure and importance. Thus, we can say that the rhythmogram provides a visual representation of the tree-like rhythmic structure of a musical piece.

The theoretically derived rhythmogram can be applied to actual performances to represent not only the rhythmic structure of the composition, but also the perceptual experience of the piece as a function of its decay rate over time. This links the mathematical idea of a filterbank to the auditory system, with its built-in mechanisms for the detection of onsets. In addition, the rhythmogram could be used to represent the sound structure of speech as well as music.

An application of the rhythmogram to represent perceptual decay is shown here in the speech utterance "tennessee air":

(image source: http://www.dcs.shef.ac.uk/~guy/pdf/icslp94.pdf) 

Instead of a low-frequency filterbank, the above rhythmogram uses a the rate of decay of memory as its y axis. Notes which are more important in the rhythmic structure would be in memory for longer; thus, it would take a higher number of seconds to decay from memory. 

This use of the rhythmogram also yields a tree-like structure, which can be represented diagramatically:
 
(image source: http://www.dcs.shef.ac.uk/~guy/pdf/icslp94.pdf) 

Using the rhythmogram, the hierarchical structure of music and speech can be derived based on signal processing methods.

## Models of Rhythm Perception
Humans seem to have remarkable ability to perceive and produce rhythm. How do we account for such abilities scientifically? What is it about our bodies and our minds that enables the perception and production of rhythm? 

Here we present two models of rhythm perception and production: the interval timing model and the coupled oscillator model. The interval timing model predicts the inter-onset interval between two events by viewing 
Again, it is not clear that the answer must be one or the other; the truth may well lie somewhere in between, or it may be a third model which differs from both interval timing and coupled oscillator models. 

An alternate model that relates rhythmic perception and production uses the Bayesian prediction framework, which predicts the probability of an event based on its prior probability, via the BayesRule. This framework has been applied to rhythm to predict rhythm production based on rhythm perception (Sadakata & Desain, 2006). 

## Interval Timing
Interval timing models of rhythmic perception and production posit that each rhythmic event is related to its previous event via a pacemaker, or accumulator. The pacemaker is a central timekeeper that sets an overall speed, or tempo, controlling the series of rhythmic events. The accumulator receives pulses from the pacemaker, and compares each interval to the interval immediately before it. 

Pacemaker and accumulator models are well supported by neural evidence. Neurological patients with lesions in the basal ganglia, such as patients with Parkinson's Disease, have trouble keeping time accurately at high levels; that is, they tend to speed up or slow down their tempo significantly over time. In contrast, patients with cerebellar damage have trouble maintaining time at smaller intervals; their timing tends to be uneven and noisy. Taken together, these results suggest that the basal ganglia may act as a pacemaker, or a central timekeeper, whereas the cerebellum acts as an accumulator which corrects each individual rhythmic event by comparing it to its previous event.

## Covariance Model
One common observation in rhythm production is that when asked to tap evenly, humans tend to produce rhythms that are slightly uneven. For example, suppose you are required to produce even taps of two taps per second. Here is a time line of your expected taps, in seconds:
 
However, it turns out that the actual measured production is as followed:

The simplest mathematical model to predict this pattern of behavior is the covariance model. The covariance model proposes that a negative correlation exists between the time interval between two taps and the time interval between the two taps immediately preceding. That is, if tn is the time interval between successive taps, i.e.

Then for every time interval tn, the next time interval, tn+1, is negatively correlated in length.
 If tn is long, then tn+1 is short; if tn is short, then tn+1 is long. (This is the negative correlation principle). 
Much of the deviations between rhythmic tapping can be modelled this way.

## CoupledOscillator
In contrast to the IntervalTiming models of rhythm, the coupled oscillator models conceive of rhythm as the result of recurring cycles of individual processes known as oscillators. An oscillator is a device which produces a recurrent output at a fixed frequency determined by its own properties, with an amplitude that depends on the amount of energy given to it. A sine wave is a simple output of an oscillator. A spring and a pendulum are both prime examples oscillators. 

A grandfather clock contains a pendulum, which is a kind of oscillator.

A simple pendulum. (image source: http://webpages.ursinus.edu/mtakats/gifcat/pendulum.html)

A spring is another example of a simple oscillator. (image source: http://webpages.ursinus.edu/mtakats/gifcat/spring.html)

The coupled oscillator model states that a set of oscillators entrain to each other - that is, their frequencies become tuned to each other, or become coupled as a result of the interaction between two oscillators.

Coupled oscillators as two symmetrical springs. (image source: http://webpages.ursinus.edu/mtakats/gifcat/springsym.html)

Coupled pendulum oscillators (image source: http://xeon.concord.org:8080/modeler/index.html)
Applied to rhythm, the coupled oscillator model states that rhythm is the result of a set of internal oscillators which entrain towards the expected rhythm. Large and Kolen (1994) and McAuley (1996) have modeled the perception and production of rhythm using coupled oscillators, where the placement of each beat within a metric structure is set by the phase of the oscillator, and individual oscillators entrain towards the recurrent beat. The use of coupled oscillators has led to some successful beat-finding algorithms. Coupled oscillator theories are computationally attractive as they can predict many observations, including the perception of meter as a recurrent rhythmic pattern, using relatively elegant mathematical models. Unfortunately, we are not yet sure of physiological bases of coupled oscillator models.

# Summary
We have provided an overview of the physic and biology of rhythm perception and production. Our perception of rhythm depends on sounds; in particular on perceptual accents or event onsets within the streams of sounds to which we are exposed in the environment. Rhythm seems to function optimally at the range of the TactuS, and can be subdivided into the smallest unit known as the TatuM. MicrotimingAndExpressiveTiming can be described using TimeMaps, which include the RhythmoGram and graphs relating score time to performance time. Models of rhythm and meter can be divided into two broad classes known as the IntervalTiming models and the CoupledOscillator models. In some cases, rhythm production can also be modeled as covariance. The Bayesian method provides a method to relate rhythm perception with production.

# References

Vijay Iyer's thesis: http://cnmat.berkeley.edu/People/Vijay/00.3%20Table%20of%20Contents.html#anchor292689

Desain & Windsor (2000) book on rhythm perception and production: http://www.nici.kun.nl/mmm/papers/rpp-book/rpp-book.html

Todd, N.P. The auditory ‘primal sketch’: A multiscale model of rhythmic grouping. Journal of New Music Research, vol. 23, pp. 25-70, 1994.

Benadon, Fernando. 2006. Slicing the beat : Jazz eighth-notes as expressive microrhythm. Ethnomusicology, 50(1), 73-98.

Desain & Honing, Perception. 2003.

Diedrichsen, Ivry & Pressing. 2003. Cerebellar and basal ganglia contributions to interval timing. http://ist-socrates.berkeley.edu/~ivrylab/pdf/dip_03.pdf

Sadakata, M., Desain, P.W.M., & Honing, H.J. (2006). The Bayesian way to relate rhythm perception and production. Music Perception, 23 (3), 267-286.

Buhusi & Meck. (2005). Nature Neuroscience 
http://www.nature.com/nrn/journal/v6/n10/pdf/nrn1764.pdf

Large & Kohlen. (1994). Connection Science, 6 (1), 177 - 208.

## Links and Downloads

AccelerandoExplorer 

Doug Eck's java simulator of oscillators - provides explanations of coupled oscillator models in rhythm. http://www.iro.umontreal.ca/~eckdoug/vibe/index.html 

Java applet demo of coupled oscillators: http://www.lon-capa.org/~mmp/applist/coupled/osc2.htm 

Peter Desain's Music, Mind, and Machine Group offers several downloadable demos on expressive timing and the Bayesian model of rhythm. http://www.nici.kun.nl/mmm/

# Quiz Items
1. What are the important features of sound that enable the perception of rhythmic beats?
1. What is the optimal range of rhythmic function? How do we know?
1. What are the axes of a rhythmogram?
1. What is microtiming?
1. What are the relative advantages and disadvantages of the interval timing model and the coupled oscillator model?
{% include unit_postamble.md %}
