---
###
# unit information: 
# This unit is about the types of research design in the field of music perception and cognition, with some practical information about how to get started in conducting your own music science research.
###
title: Research Design and Methods
number: 0
short_description: Research Design and Methods
summary: Statistics, Quantitative and Qualitative Methods, Ethical principles of interdisciplinary research.
authors: 
 - Psyche Loui
topics: [topic 1, topic 2]
test_questions:
 - question 1
 - question 2

###
# page layout:
# don't change
###
layout: unit
citations: ""
mathjax: true
---

{% include unit_preamble.md %}

# Research methods in music perception and cognition
Music perception and cognition is an interdisciplinary field concerned with applying the methods of cognitive science (experimental, computational, and neurological) — to issues and problems in the study of music. 
Ccognitive scientists use a host of methods to study perception and cognition. Every method has limitations, but different methods working together can provide powerful convergent evidence. This chapter provides an overview of the specific research methods, techniques, and experiment design considerations through which cognitive scientists explore questions about the perception and cognition of music.

# Experimental psychology

Psychology is the science of the mind. While there are many subfields of psychology, three of them -- cognitive psychology, animal psychology, and neuropsychology -- have contributed a great deal to the study of perception through experimentation. Hence, each of these subfields qualifies as experimental psychology too. As the name implies, experimental psychology is the type of psychology whereby a researcher forms a hypothesis, tests the hypothesis by requiring that a subject perform a relevant task, observes the subject's behavior, then evaluates the hypothesis in relation to the positive or negative data collected. Unsurprisingly, there are a number of methods, instruments, and techniques used by experimental psychologists to test hypotheses about perception.

Some of these methods require merely presenting the subject with a sensory stimulus and having the subject report what they see. Self-report experiments are used to study many types of perceptual phenomena. Other methods require the use of instruments to record a subject's behavior as he performs a perceptual task. This is the case in studies of eye-movements and most other neuropsychological research into the brain. But because neuroscience is the science of the brain, let us turn to an overview of neuroscientific research methods.

# Neuroscientific methods

Just as there are subfields of psychology, there are subfields of neuroscience. And cognitive neuroscience, computational neuroscience, neurology, and neurobiology are among the main subfields that have contributed to our understanding of perception. This should hardly be surprising. After all, the intelligent system we want to understand most is us -- humans. Human perception occurs as a result of information processing in several kinds of systems: sensory systems (visual, auditory, somatosensory, olfactory, and gustatory), attentional systems, memory systems (for both storage and retrieval), motor systems, etc. Not only do neuroscientists study all of these systems, they do so at every structural level of organization:

Because no one research method can answer every question at every level of processing for every system neuroscientists use a variety of methods, instruments, and techniques to study perception. If we ignore the role of the computer as a research tool in modeling how the brain works (a topic we shall deal with below), neuroscientific methods fall into two classes, invasive ones and noninvasive ones.

Invasive methods are research techniques that require the introduction of an "instrument" into a subject's brain -- a scalpel, a probe, an electrode, a finger, whatever. There are several methods of this sort. Surgery is the oldest. And an enormous amount of knowledge about the functional organization of brains has been gained through modern neurosurgery upon conscious patients. Lesion studies are another classic invasive neuroscientific method. A lesion is a "damaged" area of the brain resulting from trauma ("insult") or disease. Through observing the deficit that a lesion causes, then observing the brain (after autopsy) to localize the lesion, lesion studies have contributed a great deal to our present understanding of structure-function relations in the brain. Other invasive techniques require implanting electrodes to record the electrical activity of a neuron or population of neurons. Methods requiring this procedure include stimulation studies, electroencephalogram, evoked response potentials, and single cell recordings.

Noninvasive methods are research techniques that do NOT require the introduction of an instrument into a subject's brain. All neuroimaging methods are noninvasive ones. Of the several neuroimaging methods used to study perception, some are used to identify brain structures: conventional radiographs (X-rays), computerized tomography (CT), and magnetic resonance imaging (MRI). Others are used to identify functional areas of the brain: functional autoradiography, positron emission tomography (PET), and functional magnetic resonance imaging (fMRI). Should this alphabet soup be confusing, do not fret. In the neuroscience portion of the curriculum we shall explore these methods and their underlying assumptions in greater detail.

# Computer modeling

Many people consider 'cognitive science' to be equivalent to 'computational science'. The main reason for this is the nearly universal belief among cognitive scientists that cognition requires computation. Moreover, almost everyone within the cognitive science community considers the following belief to be the fundamental assumption upon which cognitive science itself rests: Minds are to brains what programs are to computers. Some cognitive scientists treat this view only as a metaphor, which is why it is called the computer metaphor. Many others take it to be true literally. As a result, computers play a significant role in cognitive science research. This occurs in two ways. First, computers themselves are the subject of intense (sometimes controversial) research into the nature of computation. Second, in every field of cognitive science, computers are used as a research tool to model how information processing occurs in complex systems. Our focus here is on the latter.

So, what is a computer model? Well, a computer model is a computer generated simulation of something. Computers can be used to simulate many kinds of processes -- tidal waves, weather patterns, economies, traffic jams, . . ., you name it. But while it is correct to say that what you see on the computer is an imitation of some natural, social, economic, or other process, it would NOT be correct to say that what you see is aduplicationof that natural, social, economic, or other process. To simulate is to imitate, not to duplicate. Such is the reason why you would not run for high ground upon seeing a simulation of a tidal wave.

With duplication, things are different. After all, a simulation of a tidal wave on your computer does not duplicate a tidal wave. To duplicate a wave is to create a "real" wave -- even if it is only a "mini" or scale one created under controlled conditions. While a "mini" tidal wave may not make you run for high ground either, it can at least get you wet. Similarly, to clone a frog is to duplicate it -- to create a "real" frog. To duplicate something is to reproduce its essential properties. Whenever we do this, an emulation of something has been created, not a simulation. To emulate is to duplicate, not to imitate.

One of the major controversies in cognitive science today is whether any existing computer models of human cognition go beyond mere simulation to something closer to emulation. Regardless of the correct answer is to this question, computer models, whether as simulations or emulations, are valuable research tools.

But what about perception (cognition, intelligence, or some other aspect of intelligent systems under study by cognitive scientists)? Does a computer model of a perceptual process (say vision) qualify as a simulation or an emulation? For several reasons, it is not obvious what the "correct" answer to this question is. First, if the computer metaphor is literally true, then a computer program that produces the relevant perceptual output would be an emulation of the perceptual process. Second, if the computer metaphor is merely a useful research strategy, then a computer program that produces the relevant perceptual output would only simulate the perceptual process, not emulate it. Third, to complicate matters further, whether the computer metaphor is true depends on the answer to these and other questions about the nature of computation: What does it mean to be a computer? Is your brain literally a computer or something fundamentally different? If your brain is a computer, is it a digital computer (like a Mac or a PC), or is it some other (nonclassical) type of computer? These questions highlight some of the controversies surrounding the widespread use of computers to model and to explain perception.

# Designing a research study
Many factors need to be considered when designing a research study in order to ensure that results are meaningful. When designing a research study, a few factors to decide on are the underlying theory and hypothesis, the research question at hand, the quantity and quality of data needed to produce meaningful results, and the sample size and statistical tests needed to produce meaningful results.

## Sample size and statistical power
Sample size in psychological research refers to the number of observations (cases, individuals, units) included in a selection of items to be studied. This is usually denoted N (for the study as a whole) or n (for subgroups from the study). The sample size needed for a study can be estimated from a measure of statistical power, which is how effective a statistical procedure is at identifying real differences between populations. Power ranges from 0 to 1, with values of .80 or above generally considered acceptable. 

## Variables
The different aspects that can change within an experiment are called *variables*.

*Dependent*: The dependent variable is the aspect of the experiment that is being measured.

*Independent*: The independent variable is the aspect of the experiment that is being manipulated 

*Hypothesis*: a predicted relationship between Independent Variable and Dependent Variable

*Control*: an aspect of an experiment that is not being manipulated, or is being manipulated differently from the independent variable, in order to increase reliability of the experiment.

*Random*
1. Random sampling - each member of a population is equally likely to be chosen 
1. Advantage: Generalizability (external validity)
1. Randomization with replacement
1. Randomization with constraints 

*Confounding variable*: any circumstance that changes systematically with the independent variable (Confound means puzzling)    

*Internal validity:* whether the manipulated change in the independent variable caused the change in the dependent variable (or whether something else caused the change) 

## Theories and logical reasoning

Induction (reasoning): Observation —> Pattern —> Hypothesis —> Theory (specific to general)

Deduction (reasoning): Theory —> Hypothesis —> Observation —> Confirmation (general to specific)

Theories can be Descriptive vs. Quantitative (vs. Analogical)

> A good theory accounts for most of the data, is testable, is not too restrictive, has parsimony, and is able to predict the outcome of future experiments, and the best theories help answer ultimate questions (why questions) rather than just proximate questions (what questions). (David W. Martin, _Doing Psychology Experiments_)

{% include unit_postamble.md %}
