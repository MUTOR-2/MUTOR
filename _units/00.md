---
###
# unit information: 
# This unit is about the types of research design in the field of music perception and cognition, with some practical information about how to get started in conducting your own music science research.
###
title: Research Design and Methods
number: 0
short_description: Research Design and Methods
summary: Statistics, Quantitative and Qualitative Methods, Ethical principles of interdisciplinary research.
authors: 
 - Psyche Loui
topics: [experimental psychology, neuroscience, computer modeling]
test_questions:
 - Suppose you hypothesize that musical harmony is a culturally determined construct. How would you design a study to test this research question?
 - What are some possible confounds with such a research design, and how could you eliminate as many confounds as possible given the design of your experiment?

###
# page layout:
# don't change
###
layout: unit
citations: ""
mathjax: true
---

{% include unit_preamble.md %}

# Research Methods in Music Perception and Cognition

Music perception and cognition is an interdisciplinary field concerned with applying the methods of cognitive science (experimental, computational, and neuroscientific) â€” to issues and problems in the study of music. 
Cognitive scientists use a host of methods to study perception and cognition. 
Every method has limitations, but cognitive scientists use different methods to be able to provide powerful convergent evidence the questions they investigate. 
This chapter provides an overview of the terminology, research methods, and experimental techniques through which cognitive scientists investigate questions about the perception and cognition of music.

# Experimental Psychology

Psychology is a branch of science concerned with studying the mind and behavior. 
While there are many subfields of psychology, three of them -- cognitive psychology, animal psychology, and neuropsychology -- have contributed a great deal to the study of perception through experimentation. 
Unlike other areas of psychology such as social psychology that attempt to draw conclusions about human behavior by just observing the world, these three areas of psychology often employ some form of active manipulation in their research.
Hence, each of these subfields falls under the general umbreall of experimental psychology. 

As the name implies, experimental psychology is a field of psychology whereby a researcher seeks to understand the world using the scientific method.
Typically, this consists of observing the world in order to come up with some sort of theory or scientific proposition about the world (Future Cite Guest ).
A theory will often make causal claims about how the world works.
For example, the theory of gravity would claim that the reason that things fall toward the floor when you let go of them is because of a force of gravity.
Gravity is proposed as a property of the world that has regular and predictable patterns that can be observed and studied.

Unlike working with physical objects, experimental psychologists attempt to make causal claims about the mind and behavior.
They also form hypotheses, test these hypotheses by  requiring subjects to  perform a task, then observe the subject's behavior. 
After seeing how people's behavior changes in relation to some sort of manipulation, they will then think critically about how what they saw in their experiment aligns with their intial ideas.
Outcomes of experiments are evaluated in relation to the positive or negative data collected. 
Unsurprisingly, there are numerous methods, instruments, and techniques used by experimental psychologists to test hypotheses about perception.

Some of methods used by psychologists require presenting participants with a stimulus and then having the subject report on what they experienced. 
This type of design is referred to as **self-report** since it requires the participant to explicitly tell the experimenter what they experienced.
While this type of design is often used in experimental designs, it is far from perfect.
Participants in experiments are always not the best at describing what they experienced and sometimes when working with people, they choose not to be entirely accurate if they're being asked about a sensitive topic.

Contrasting to self-rport, other methods require the use of instruments to record a subject's behavior as they perform a perceptual task. 
Examples of this include studies that track measurements that are typically beyond the control of the participant.
For example it is quite difficult to consciously control eye-movements, heart rate, electrodermal activity, or any measure having to deal with brain activity. 
Experimental psychologists use these **implicit measures** of data collection for many reasons.
For example, in studies seeking to understand aspects of musical emotion, you could imagine that it might be distracting to a participant if every ten seconds they are asked how they feel when listening to a music using **self-report**.
Instead, it might be better to play them music and use an implicit measure like functional magnetic resonance imaging (fMRI) to measure their brain activity as they listen.

These types of implicit, physiological measures are quite common in the study of music perception and cognition.
For that reason, let us next turn to an overview of neuroscientific research methods.

# Neuroscientific Methods

Given the complexity of the human brain and behavior, it should come no surprise that there are numerous fields that all attempt to understand how the brain works. 
Just as there are subfields of psychology, there are subfields of neuroscience. 
Cognitive neuroscience, computational neuroscience, neurology, and neurobiology are among the main subfields that have contributed to our understanding of perception. 

Human perception occurs as a result of information processing in several kinds of systems: sensory systems (visual, auditory, somatosensory, olfactory, and gustatory), attentional systems, memory systems (for both storage and retrieval), and motor systems to name a few. 
Not only do neuroscientists study all of these systems, but they do so at every structural level of organization.
Because no one research method can answer every question at every level of processing for every system neuroscientists use a variety of methods, instruments, and techniques to study perception. 
If we ignore the role of the computer as a research tool in modeling how the brain works (a topic we shall deal with below), neuroscientific methods fall into two classes, invasive ones and noninvasive ones.

**Invasive methods** are research techniques that require the introduction of an "instrument" into a subject's brain.
These methods are invasive because the researchers use tools such as scalpels, a probes, or electrode to actually come in contact with the brain.
There are several methods of this sort and fortunatly for human subjects have become safer over time.

Surgery is the oldest and is responsible for an enormous amount of knowledge about the functional organization of the brain through modern neurosurgery upon conscious patients. 
Since the brain does not have any nerve endings, it is possible to actively manipulate someone's brain while they are awake with their skull cut open.
Here, the researcher can stimulate the brain using something like an electrode to invoke a response in their patient.
This type of approach is not common within neuroscientific studies, but can lead to direct understanding of what manipulations of the brain affect certain aspects of perception.

Lesion studies are another classic invasive neuroscientific method used to study the brain. 
A lesion is a "damaged" area of the brain resulting from trauma ("insult") or disease. 
To give an example, an individual might experiene damage to their brain after having a stroke.
If the stroke damages an area of the brain that is responsible for something in specific, for example speech production, then damage to this area could be linked to speech processing.

Over a century ago, in order to do this type of study the researchers would have to link behavior to brain lesions after the patient had died using an autopsy.
Strokes are not the only way this would have happened.
One of the classic examples that lead to our early understandings of the brain is the case of [Phineas Gage]() who survived a traumatic work accident then went on to experience severe changes in their behavior.
In present day, we often do not have to wait until someone dies due to different neuroimaging techniques.
Techiques such as  electroencephalagraphy, fMRI, and transcranial magnetic stimulation (TMS) allow for similar observation of the brain without extreme events happening. 

> ---- edit line

Other invasive techniques require implanting electrodes to record the electrical activity of a neuron or population of neurons. 

Noninvasive methods are research techniques that do NOT require the introduction of an instrument into a subject's brain. 
All neuroimaging methods are noninvasive ones. 
Of the several neuroimaging methods used to study perception, some are used to identify brain structures: conventional radiographs (X-rays), computerized tomography (CT), and magnetic resonance imaging (MRI). 
Others are used to identify functional areas of the brain: functional autoradiography, positron emission tomography (PET), and functional magnetic resonance imaging (fMRI). 
Should this alphabet soup be confusing, do not fret. 
In the neuroscience portion of the curriculum we shall explore these methods and their underlying assumptions in greater detail.

# Computer modeling

Many people consider 'cognitive science' to be equivalent to 'computational science'. 
The main reason for this is the nearly universal belief among cognitive scientists that cognition requires computation. 
Moreover, almost everyone within the cognitive science community considers the following belief to be the fundamental assumption upon which cognitive science itself rests: Minds are to brains what programs are to computers. 
Some cognitive scientists treat this view only as a metaphor, which is why it is called the computer metaphor. 
Many others take it to be true literally. 
As a result, computers play a significant role in cognitive science research. 
This occurs in two ways. 
First, computers themselves are the subject of intense (sometimes controversial) research into the nature of computation. 
Second, in every field of cognitive science, computers are used as a research tool to model how information processing occurs in complex systems. 
Our focus here is on the latter.

So, what is a computer model? 
Well, a computer model is a computer generated simulation of something. 
Computers can be used to simulate many kinds of processes -- tidal waves, weather patterns, economies, traffic jams, . . ., you name it. 
But while it is correct to say that what you see on the computer is an imitation of some natural, social, economic, or other process, it would NOT be correct to say that what you see is a duplication of that natural, social, economic, or other process. 
To simulate is to imitate, not to duplicate. 
Such is the reason why you would not run for high ground upon seeing a simulation of a tidal wave.

With duplication, things are different. 
After all, a simulation of a tidal wave on your computer does not duplicate a tidal wave. 
To duplicate a wave is to create a "real" wave -- even if it is only a "mini" or scale one created under controlled conditions. 
While a "mini" tidal wave may not make you run for high ground either, it can at least get you wet. 
Similarly, to clone a frog is to duplicate it -- to create a "real" frog. 
To duplicate something is to reproduce its essential properties. 
Whenever we do this, an emulation of something has been created, not a simulation. 
To emulate is to duplicate, not to imitate.

One of the major controversies in cognitive science today is whether any existing computer models of human cognition go beyond mere simulation to something closer to emulation. 
Regardless of the correct answer is to this question, computer models, whether as simulations or emulations, are valuable research tools.

But what about perception (cognition, intelligence, or some other aspect of intelligent systems under study by cognitive scientists)? 
Does a computer model of a perceptual process (say vision) qualify as a simulation or an emulation? 
For several reasons, it is not obvious what the "correct" answer to this question is. 
First, if the computer metaphor is literally true, then a computer program that produces the relevant perceptual output would be an emulation of the perceptual process. 
Second, if the computer metaphor is merely a useful research strategy, then a computer program that produces the relevant perceptual output would only simulate the perceptual process, not emulate it. 
Third, to complicate matters further, whether the computer metaphor is true depends on the answer to these and other questions about the nature of computation: What does it mean to be a computer? 
Is your brain literally a computer or something fundamentally different? 
If your brain is a computer, is it a digital computer (like a Mac or a PC), or is it some other (nonclassical) type of computer? 
These questions highlight some of the controversies surrounding the widespread use of computers to model and to explain perception.

# Designing a research study

Many factors need to be considered when designing a research study in order to ensure that results are meaningful. 
When designing a research study, a few factors to decide on are the underlying theory and hypothesis, the research question at hand, the quantity and quality of data needed to produce meaningful results, and the sample size and statistical tests needed to produce meaningful results.

## Sample size and statistical power
Sample size in psychological research refers to the number of observations (cases, individuals, units) included in a selection of items to be studied. This is usually denoted N (for the study as a whole) or n (for subgroups from the study). The sample size needed for a study can be estimated from a measure of statistical power, which is how effective a statistical procedure is at identifying real differences between populations. Power ranges from 0 to 1, with values of .80 or above generally considered acceptable. 

## Variables

The different aspects that can change within an experiment are called *variables*.

*Dependent*: The dependent variable is the aspect of the experiment that is being measured.

*Independent*: The independent variable is the aspect of the experiment that is being manipulated 

*Hypothesis*: a predicted relationship between Independent Variable and Dependent Variable

*Control*: an aspect of an experiment that is not being manipulated, or is being manipulated differently from the independent variable, in order to increase reliability of the experiment.

*Randomization*
1. Random sampling - each member of a population is equally likely to be chosen 
1. Advantage: Generalizability (external validity)
1. Randomization with replacement
1. Randomization with constraints 

*Confounding variable*: any circumstance that changes systematically with the independent variable (Confound means puzzling)    

*Internal validity:* whether the manipulated change in the independent variable caused the change in the dependent variable (or whether something else caused the change) 

## Theories and logical reasoning

Induction (reasoning): Observation â€”> Pattern â€”> Hypothesis â€”> Theory (specific to general)

Deduction (reasoning): Theory â€”> Hypothesis â€”> Observation â€”> Confirmation (general to specific)

Theories can be Descriptive vs. Quantitative (vs. Analogical)

> A good theory accounts for most of the data, is testable, is not too restrictive, has parsimony, and is able to predict the outcome of future experiments, and the best theories help answer ultimate questions (why questions) rather than just proximate questions (what questions). (David W. Martin, _Doing Psychology Experiments_)

{% include unit_postamble.md %}
