<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.14 (458244)"/><meta name="keywords" content="notes"/><meta name="altitude" content="11"/><meta name="author" content="razorpsyche"/><meta name="created" content="2014-01-31 21:58:04 +0000"/><meta name="latitude" content="42.33749735999999"/><meta name="longitude" content="-71.11071840999998"/><meta name="updated" content="2019-09-13 15:40:07 +0000"/><title>MPC Day 4: Auditory scene analysis and auditory illusions. 2/4/14</title></head><body><div><span style="font-size: 18px;"><span style="font-size: 18px; font-family: Arial; font-weight: bold; text-decoration: underline;">Auditory scene analysis and auditory illusions</span></span></div><div><span style="font-size: 18px; font-family: Arial;">Housekeeping: October 11 Neuroarts forum starts 1pm</span></div><div><span style="font-size: 18px; font-family: Arial;">Soundscapes and Sound Ecology 11:50-12:00pm</span></div><div><span style="font-size: 18px; font-family: Arial;">Auditory Scene Analysis and Auditory Stream Segregation 12:00-12:30pm  </span></div><div><span style="font-size: 18px; font-family: Arial;">     Bottom-Up vs. Top-Down Perspectives</span></div><div><span style="font-size: 18px; font-family: Arial;">        Bottom-Up: frequencies, amplitude </span></div><div><span style="font-size: 18px; font-family: Arial;">            </span><span style="font-size: 18px; font-family: Arial;">harmonics_demo.maxpat</span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">    LoudnessPerception</span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">        IntenSity</span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">        DeciBels</span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">        LoudnessAndFrequency</span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">        PHones</span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">        SOnes</span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">        LoudnessOfMultiplePitches</span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">        LoudnessAndSpectra</span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">        LoudnessAndTime</span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">        LoudnessCues</span></div><div><span style="font-size: 18px; font-family: Arial;">     Primitive vs. Schema-based perspectives </span></div><div><span style="font-size: 18px; font-family: Arial;">    Streams: segregation and fusion</span></div><div><span style="font-size: 18px; font-family: Arial;">        Musical allusions </span></div><div><span style="font-size: 18px; font-family: Arial;">Perceptual Illusions </span><span style="font-size: 18px; font-family: Arial;">: When percept does not match stimulus: Top down ~= Bottom Up   </span><span style="font-size: 18px; font-family: Arial;"> 12:30-1:00pm</span></div><div><span style="font-size: 18px; font-family: Arial;">        Gestalt Psychology </span></div><div><span style="font-size: 18px; font-family: Arial;">        AudioVisual Interaction</span></div><div><span style="font-size: 18px; font-family: Arial;">            VentriloquismEffect</span></div><div><span style="font-size: 18px; font-family: Arial;">            McGurkEffect </span></div><div style="margin-left: 40px;"><span style="font-size: 18px; font-family: Arial;">Auditory Illusions: 1:00-1:25pm</span><span style="font-size: 18px; font-family: Arial;">                    </span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">Speech to Song Illusion </span><span style="font-size: 18px;"><a href="http://deutsch.ucsd.edu/psychology/pages.php?i=212" style="font-size: 18px; font-family: Arial;">http://deutsch.ucsd.edu/psychology/pages.php?i=212</a></span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">Tritone Illusion </span><span style="font-size: 18px;"><a href="http://deutsch.ucsd.edu/psychology/pages.php?i=206" style="font-size: 18px; font-family: Arial;">http://deutsch.ucsd.edu/psychology/pages.php?i=206</a></span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">Octave Illusion </span><span style="font-size: 18px;"><a href="http://deutsch.ucsd.edu/psychology/pages.php?i=202" style="font-size: 18px; font-family: Arial;">http://deutsch.ucsd.edu/psychology/pages.php?i=202</a></span></div><div style="margin-left: 80px;"><span style="font-size: 18px; font-family: Arial;">Laurel vs Yanny </span><span style="font-size: 18px;"><a href="https://web.northeastern.edu/mindlab/2018/09/05/laurel-vs-yanny/" style="font-size: 18px; font-family: Arial;">https://web.northeastern.edu/mindlab/2018/09/05/laurel-vs-yanny/</a></span><span style="font-size: 18px; font-family: Arial;">  </span><span style="font-size: 18px; font-family: Arial;">    </span></div><div style="margin-left: 40px;"><span style="font-size: 18px;"><br/></span></div><div><span style="text-decoration: underline;">Auditory scene analysis and auditory illusions</span></div><div>Soundscapes and Sound Ecology</div><div>
Auditory Scene Analysis and Auditory Stream Segregation 
</div><div>     Bottom-Up vs. Top-Down Perspectives</div><div>     Primitive vs. Schema-based perspectives </div><div>        Musical allusions </div><div>        Perceptual Illusions</div><div>        Gestalt Psychology </div><div>        AudioVisual Interaction</div><div>            VentriloquismEffect</div><div>            McGurkEffect </div><div><br/></div><div>Soundscape</div><div><a href="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/Coqui%CC%81es1.aiff">Coquíes1.aiff</a></div><div>Field recording made in Hatillo, Puerto Rico. Singing coquís overnight. </div><div><a href="http://www.youtube.com/watch?v=54-FzuE-w0U"><img src="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/d242148ed76d032b3ad852fa9306a0e0.jpeg" height="455" width="650"/>http://www.youtube.com/watch?v=54-FzuE-w0U</a></div><div><br/></div><div>What’s in a stream? </div><div>Segregation vs. Integration </div><div><br/></div><div>Unit2Introduction</div><div>In the previous unit we introduced various physical properties of the sound stimulus. We saw that frequency, ampltude, phase, temporal envelopes, and spectral envelopes are all useful descriptors of sounds, and that different visual representations may contain useful information about these sound properties.</div><div>
But how do these physical descriptions relate to how you actually hear? In other words, how does our perception of a physical stimulus depend upon its context?</div><div>
In this unit we are primarily concerned with <span style="font-weight: bold;">how sounds interact with the world, and the ways with which they can be influenced by our long-term knowledge and understanding</span>. One important distinction we make is between the Bottom-Up and Top-Down perspectives of perception. Bottom-Up processing gives rise to some signal processing (such as the extraction of frequency, reverberation, and some issues explored in Elementary Psychoacoustics. Loudness Perception and room acoustics are two topics that receive important information from the Bottom-Up perspective, but are heavily dependent on Top-Down knowledge as well. This unit will also investigate the cases involving mismatch between Top-Down and Bottom-Up signals, which give rise to the Precedence Effect, the Ventriloquism Effect, and other perceptual phenomena and illusions.</div><div><br/></div><div>SoundEcology </div><div>In the natural world, sounds rarely occur in complete isolation. Our sonic environment, or <a title="External link to http://en.wikipedia.org/wiki/Soundscape" href="http://en.wikipedia.org/wiki/Soundscape">soundscape</a>, consists of combinations of sounds that may include intentional signals as well as sounds that are produced as a result of interactions with the environment. Aspects of the soundscape may contain useful information about the world. One example is reverberation, which is produced by the interaction of a sound with its environment, and the listener can use reverb to infer information about the size and shape of the room.</div><div><br/></div><div>
An analogy of the auditory system's ability to construct a picture of the world can be drawn from the following scenario.</div><div>
<img src="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/2461E3AC-F6BB-4BEE-B1B0-4A9F1FF7D87A.jpg" height="354" width="472"/><br/></div><div>
Here is the surface of an irregularly-shaped lake, with two inlets coming out of its side. A fishing bob is floating on each of the inlets.</div><div>
Now suppose you are only allowed to look at the fishing bobs, but you must answer the following questions:</div><div>
1. How many ducks are there?</div><div>
2. Which way are the people swimming?</div><div>
3. Which is closer to you, the sailing boat or the swimmers?</div><div>
When only given information from the two fishing bobs, it may seem impossible to answer all these questions. Surprisingly, our auditory system seems to have little trouble given a similar problem. Although we have only two ears, the information we can glean from these two input receivers is powerful enough to infer many things about the environment. <span style="text-decoration: underline;">The auditory system's ability to use sounds to understand the environment is termed by Bregman (1990/1994) as </span><span style="font-weight: bold; text-decoration: underline;">Auditory Scene Analysis</span><span style="text-decoration: underline;">.</span></div><div>Top-Down</div><div>While Bottom-Up perspectives of perception refer to the stimulus-driven processing of sounds beginning from the periphery of the auditory system, Top-Down perspectives begin with the higher-level mental processes that are accessible to the human mind. Top-Down processes may include our knowledge of the context of a piece of music, our memory of related musical pieces, the cognitive effort of paying attention to certain aspects of the stimulus, and our emotional state during the perception of music.</div><div><br/></div><div>
One example of top-down knowledge is the understanding of a musical allusion - i.e. knowing the source material from which another musical work is quoting. This can often influence how the music is perceived. A famous case of a musical allusion is in the third movement of Mahler's First Symphony.</div><div>
Children's song "Frere Jacques” <a href="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/frere-jacques.mp3">frere-jacques.mp3</a></div><div>
The opening measures of Mahler's Symphony No. 1 in D, "Titan”. <a href="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/mahler1.mp3">mahler1.mp3</a></div><div>
Mahler quoted the melody from the traditional children's song Frere Jacques, but transposed it into the minor mode. An especially eerie and intriguing effect is achieved when the audience recognizes the source of the musical quotation. Thus the perception of the mood of this piece is enhanced by top-down knowledge.</div><div
/><div>
Another example is the effects of visual information on the perception of musical sounds. In an audiovisual environment, information received by the visual and auditory systems interact in important ways. The use of sound effects and soundtracks in the film industry relies heavily on AudioVisual Interaction, and much research has shown that auditory, visual, and tactile sensations influence each other in perceptual as well as cognitive processing.</div><div><br/></div><div>Perceptual Illusions</div><div>Top-Down perspectives influence Bottom-Up processing in many important ways. Compare the heights of the two people in the following picture.</div><div><img src="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/E0307635-C1B3-4EAD-8D03-58751F98BCAA.jpg" height="341" width="640"/><img src="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/E0307635-C1B3-4EAD-8D03-58751F98BCAA.jpg" height="341" width="640"/><br/></div><div>Although the two persons are identical in actual size, the figure closer to the bottom of the picture is perceived as being smaller than the other person. This is because of top-down inference of the higher person being further away, and our knowledge of linear perspective playing a role in the picture leads us to infer that the further figure is larger. If the background context is removed, the illusion disappears and we perceive both people as of similar size.</div><div>
<img src="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/3F8E21AC-0F20-44B8-BB8B-A332B8C238C4.jpg" height="341" width="640"/><br/></div><div>Perceptual illusions like this one often rely on the <span style="text-decoration: underline;">unconscious inference</span> of contextual cues, which is based on top-down knowledge of naturalistic scenes.</div><div><br/></div><div><span style="font-weight: bold;">Demos</span></div><div>Top down modulation / Auditory stream segregation:</div><div>interleaved melodies: </div><div><a href="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/ASA%20stream%20segregation%20demo%20Track05.mp3">ASA stream segregation demo Track05.mp3</a></div><div>Sine wave speech:</div><div><a href="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/aslq1813SWS.wav">aslq1813SWS.wav</a></div><div><a href="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/aslq1813.wav">aslq1813.wav</a></div><div><span style="font-size: 12px; font-family: Helvetica;">Attention and learning can play a role when we extract some of the components of mixtures for the purposes of pattern analysis.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- distinguishing timbre depends on scene analysis - <span style="font-size: 12px; font-family: Helvetica; text-decoration: underline;">grouping components of signal into a perceptual stream.</span></span></div><div><span style="font-size: 12px; font-family: Helvetica;">
 - examples: timbre perception, speech perception - can occur with the auditory system putting spectral regions together despite acoustic cues telling auditory system to segregate them. Primitive segregation is not a sufficient explanation. Depends on listener's knowledge of their language.</span></div><div><span style="font-size: 12px; font-family: Helvetica;"
/></div><div><span style="font-size: 12px; font-family: Helvetica;">
<span style="font-size: 12px; font-family: Helvetica; font-style: italic;">Nature of Primitive and Schema-Based Organization</span></span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Auditory scene analysis can use a more sophisticated knowledge of the signal than just spectral and sequential grouping cues.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
2 different processes: primitive scene analysis vs. schema-driven construction of descriptions.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- "primitive" - simpler, probably innate, driven by bottom-up incoming acoustic data</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- "schema-driven" or "hypothesis-driven" - involves activation of stored knowledge of familiar patterns or schemas in the acoustic environment and of a search for confirming stimulation in the auditory input.  involves selecting the right components to match stored schemas</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Basically - top-down vs. bottom-up, according to info processing theory.</span></div><div><span style="font-size: 12px; font-family: Helvetica;"
/></div><div><span style="font-size: 12px; font-family: Helvetica;">
<span style="font-size: 12px; font-family: Helvetica; font-style: italic;">Properties that may distinguish the two systems</span></span></div><div><span style="font-size: 12px; font-family: Helvetica;">Assumption: Primitive segregation process employs neither voluntary attention nor past learning. If the process involves either attention or learning, we'll call it schema-driven.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Attention encompasses 2 facts:</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
1. selecting part of currently available sensory information for more detailed processing. i.e. making some sensory information available to consciousness, e.g. dichotic listening. Easier to segregate if signals are different in pitch and/or location... - </span><span style="font-size: 12px; font-family: Helvetica; font-style: italic;">trying</span><span style="font-size: 12px; font-family: Helvetica;"> is central to attention.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
2. But we're never sure if what we hear is an effect of primitive or schema-driven processes, except by testing to see if </span><span style="font-size: 12px; font-family: Helvetica; font-style: italic;">trying</span><span style="font-size: 12px; font-family: Helvetica;"> makes a difference.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Attention can:</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
1. give you a more detailed awareness of things (allocating mental resources), or</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
2. be taxed when you're trying to pay attn to too many things at once (there is a limited pool of resources.)</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
3. as you become highly practiced in a task, it comes to require less attention. e.g. driving. (attention is involved in novel coordination of separate skills.)</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
So, attn is an effortful process that coordinates existing schemas in a new task and in which the coordinating process can control only a limited set of mental resources.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
one example of attention in automatic processes: training as a psychophysical subject; knowing what to listen for (top down modulation)</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Primitive analysis - employs heuristics found in the environment - e.g. frequency proximity, correlations of changes in acoustic properties, spectral similarity - Gestalt grouping.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Schema-based process - uses knowledge (schema) about specific domains, requiring computations of how to deal with particular regularities in our environment; eg. grammatical and lexical patterns required to form a sentence. Dealing with environmental regularity.</span></div><div><span style="font-size: 12px; font-family: Helvetica;"
/></div><div><span style="font-size: 12px; font-family: Helvetica;">
Nativism-empricism controversy in psychology: can schemas be innate? If so, how complex can innate schemas be? Either way, they must collaborate smoothly.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- one example of schema induction: pattern recognition. Auditory: temporal patterns. After some exposure to parts of a (temporal) pattern, schema can be primed to detect later elements. very powerful technique lor. - <span style="font-size: 12px; font-family: Helvetica; font-weight: bold;">Expectation facilitates perception.</span></span></div><div><span style="font-size: 12px; font-family: Helvetica;">
The human brain probably has some method of combining the benefits provided by the two systems.</span></div><div><span style="font-size: 12px; font-family: Helvetica;"
/></div><div><span style="font-size: 12px; font-family: Helvetica;">
<span style="font-size: 12px; font-family: Helvetica; font-style: italic;">How do we know they should be distinguished?</span></span></div><div><span style="font-size: 12px; font-family: Helvetica;">
children 1.5-3.5 years old show evidence of auditory stream segregation (Laurent Demany)</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
temporal coherence boundary and fission boundaries can be modulated if the pitch difference between upper and lower tones are changed.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
top down knowledge of, or familiarity with, a background melodic sequence does not help you distinguish a target melody from the background. (Dowling)</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Westminster chimes illusion (Deustch) - displacing notes of a melody by octaves disrupts your recognition of the tune, but not if you know what the tune is.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Another diff between primitive and schema-governed segregation: temporal scope.  Schema-based processes can span longer time intervals</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- e.g. phonemic restoration is sensitive to semantic stem completion type stuff.</span></div><div><span style="font-size: 12px; font-family: Helvetica;"
/></div><div><span style="font-size: 12px; font-family: Helvetica;">
<span style="font-size: 12px; font-family: Helvetica; font-style: italic;">Does Learning Affect Streaming?</span></span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Yes. See more of Dowling's work on interleaved melodies.</span></div><div><span style="font-size: 12px; font-family: Helvetica;"
/></div><div><span style="font-size: 12px; font-family: Helvetica;"
/><span style="font-size: 12px; font-family: Helvetica; font-style: italic;">Do Regular Patterns Form More Coherent Streams?</span><span style="font-size: 12px; font-family: Helvetica;"> Jones' Rhythmic Theory of Attention</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- research on recognition and memorization of short melodic and rhythmic patterns.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- evidence that human attention can be deployed in a rhythmic manner.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- as we listen to a pattern of sounds, the process of attention is capable of anticipating the position of the next sound on the time dimension, the pitch dimension, and others (what others??) - prepare to pick up the next sound.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- a predictable sequence allows it components to be caught in the net of attention, whereas unpredictable elements may be lost.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- rules of rhythmic attention: hierarchy of embedded units, each unit forming a component of a larger unit. sequential units that group together to form higher-order unit must have particular relationships: e.g. repeating after a (relatively simple) transformation.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
   - i.e. attention of the repetition-with-variation type</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- Jones's theory seems biased for schema-driven processes.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- schemas are fundamental to the memorizing process.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Jones's theory predicts:</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
1. Learning will affect stream segregation.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
2. Regular patterns will be found to form more coherent streams.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
3. Streams are created by attention and by the search for regular patterns in the stimulus sequence.</span></div><div><span style="font-size: 12px; font-family: Helvetica;"
/></div><div><span style="font-size: 12px; font-family: Helvetica;">
<span style="font-size: 12px; font-family: Helvetica; font-style: italic;">Does the auditory system track and project trajectories?</span></span></div><div><span style="font-size: 12px; font-family: Helvetica;">
analogy to vision: smooth curves and straight lines</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
<span style="font-size: 12px; font-family: Helvetica; font-style: italic;">Evidence that the system does not project trajectories:</span></span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- Deutsch's scale illusion</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- detection of ascending glide (up-up) is no better than complex glide (down-up) - Steiger and Bregman - no evidence that the auditory system could measure slopes.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- but Ciocca-Bregman's experiment on illusory continuity of glides behind a loud noise burst - found that the perceptual restoration of the missing portion of the glide was better when the glide segments that entered and exited from the noise were aligned on a common trajectory... but the two expts were very different.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- Many other reports of failure to find a trajectory-extrapolating effect - Gary Dannenbring (McGill), Chris Darwin</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
<span style="font-size: 12px; font-family: Helvetica; font-style: italic;">Evidence that the system projects trajectories:</span></span></div><div><span style="font-size: 12px; font-family: Helvetica;">
a few cases are cited, but there is an alternative explanation in every case.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
- trajectory-based integration of streams: exptrapolation of the mirror-image glide through noise, which can be explained by Interpolation and Extrapolation. Expts by Ciocca and Bregman could not distinguish between different possible restoration mechanisms.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Order of unidirectional sequences is easier to report - but this is not due to trajectory-based principle of primitive grouping, but to other factors.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Results explainable by other factors.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
van Noorden, Divenyi &amp; Hirsh, etc. etc. experiments using phonemic restoration and other methods</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
e.g. explanation in terms of the frequency proximity of the nearest temporal parts of successive vowels.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
More explanations:</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
1. the auditory system may simply detect local discontinuities rather than forming rules that predict that trajectory.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
2. Unidirectional ascending or descending sequences may simply be easier for our memories to encode and recall than irregular sequences.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
3. It is possible that there is a trajectory-following mechanism but that it is based on schemas and requires prior learning, and, in some cases, a conscious attempt to track the trajectory.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
4. There is an absence of competition based on frequency proximity in glissandi</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Comparison with Vision - existence of visual momentum as an evolutionary adaptation of the visual system to regularities in the physical environment in the manner described so elegantly by Roger Shepard. Can we formulate a similar argument in audition? I think not.  Argument against pitch-change inertia.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
<span style="font-size: 12px; font-family: Helvetica; font-style: italic;">"Is auditory attention inherently rhythmical?"</span></span></div><div><span style="font-size: 12px; font-family: Helvetica;">three possibilities:</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
1. rhythmic regularity has no effect on the inclusion of sounds in streams.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
2. although regularity promotes inclusion in the same stream, it is not essential.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
3. temporal regularity is so essential that there can be no stream segregation without it.</span></div><div><span style="font-size: 12px; font-family: Helvetica;"
/></div><div><span style="font-size: 12px; font-family: Helvetica;">
Evidence that rhythm favours segregation - mostly from Jones et al. and Dowling.</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Segregation occurs with temporal irregular sequences</span></div><div><span style="font-size: 12px; font-family: Helvetica;">
Predictability can help hold a stream together, but is probably not a requirement for obtaining stream segregation effects.</span></div><div><span style="font-size: 12px; font-family: Helvetica;"
/></div><div><span style="font-size: 12px; font-family: Helvetica;">
"Are streams created by attention?" all the factors that influence primitive groping are really influencing attention, or, as Helmholtz would say, due to inattention. but we think there are still two proposed processes, one primitive and preattentive, and the other schema-driven and attention-directing. They do not always react the same way to the same acoustic variables. Sometmes impossible to separate the two processes. But when things are in the same stream for the primitive system, it's still possible to segregate things using the schema-driven process.</span></div><div>AudioVisualInteraction</div><div>PsycheLoui, 9 June 2006 (created 2 June 2006)</div><div>So far we have discussed the auditory system's processing of sounds and its various dimensions, such as frequency and amplitude. We have also started to touch on the interactions between different auditory dimensions, such as the interaction between frequency and loudness as shown by the Fletcher-Munson curves. However, music is not just a series of auditory stimuli. Information from other modalities, such as vision and touch, also play important roles in the musical experience. What is seen can influence what is heard, and this is especially true from a Top-Down perspective.</div><div><br/></div><div>
Soundtracks in movies offer one case of auditory perception enhancing visual perception. The use of sound effects and soundtracks in the film industry relies heavily on AudioVisualInteraction, and much research has shown that auditory, visual, and tactile sensations influence each other in perceptual as well as cognitive processing. In one particular study by Marilyn Boltz (2001), video clips were paired with positive, neutral, and negative soundtracks and shown to study participants. After viewing the films, participants were asked to rate how well certain adjectives described the content of the video (e.g. "What adjective best describes the man's intentions for following the womann? benevolent / malevolent")</div><div>
Participants rated videos with positive soundtracks as being better described by positive adjectives, and videos with negative soundtracks using negative adjectives. Videos with no soundtracks were rated as relatively neutral. This study shows strong effects of musical stimuli on the perception of meaning and emotional content of visual scenes.</div><div>
<img src="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/90ED216A-A51F-4B4A-8B64-F26C671BFC9F.jpg" height="303" width="478"/><br/></div><div>
(image source &amp; reference: <a title="External link to http://caliber.ucpress.net/doi/pdfplus/10.1525/mp.2001.18.4.427" href="http://caliber.ucpress.net/doi/pdfplus/10.1525/mp.2001.18.4.427">http://caliber.ucpress.net/doi/pdfplus/10.1525/mp.2001.18.4.427</a>)</div><div>
M.G. Boltz. 2001. Musical Soundtracks as a Schematic Influence on the Cognitive Processing of Filmed Events. <span style="font-style: italic;">Music Perception</span> Summer 2001.</div><div
/><div>
In the real world, events in the visual and auditory modalities are usually correlated. For example, seeing the downward motion of a drummer is usually correlated with hearing the drum beat. In some relatively infrequent cases, however, information from the visual and auditory modalities do not match. When this mismatch occurs, the brain engages interesting processes from a Top-Down perspective to make up for these discrepancies between the senses.</div><div>VentriloquismEffect</div><div>PsycheLoui, 5 June 2006 (created 30 May 2006)</div><div>When an audio-visual mismatch occurs, the brain may decide to reinterpret information from the auditory system such that it matches the visual modality. In a sense, the brain makes the auditory system obey the visual system, so that the resulting percept is of a single representation of a multimodal stimulus. The phenomenon of visual information overriding auditory information is known as <span style="font-weight: bold;">visual capture</span>.</div><div>
One demosntration of AudioVisualInteraction, especially of the visual capture of auditory stimuli, is the case of ventriloquism.</div><div>
<a title="External link to http://www.paulzerdin.com/" href="http://www.paulzerdin.com/">http://www.paulzerdin.com/</a></div><div>
<a href="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/babyvent.mpg">babyvent.mpg</a></div><div>
(video source: <a title="External link to http://www.paulzerdin.com" href="http://www.paulzerdin.com/">http://www.paulzerdin.com</a>)</div><div>
(see also http://video.google.com/videoplay?docid=8126344216754570223&amp;q=ventriloquist+is%3Afree+duration%3Ashort&amp;pr=goog-sl)</div><div>
Ventriloquism relies on the brain's tendency to integrate auditory information to agree with the visual system. In a ventrilquism act, the ventriloquist moves the puppet's mouth while talking without moving his/her lips. Because the puppet is moving while the performer is not, we localize the sound source to the puppet instead of the ventriloquist; thus, we perceive the speech as coming from the puppet.</div><div><br/></div><div>McGurkEffect</div><div>PsycheLoui, 2 June 2006 (created 1 June 2006)</div><div><div>When the auditory and visual systems are uncorrelated, visual capture (such as the VentriloquismEffect) is a typical result. There are some cases, however, when visual capture does not occur. Instead, the brain takes input from both auditory and visual systems, and integrates them to create a unique multimodal percept which is different from either the visual or auditory stimulus alone.</div><div
/><div>
In the lab it is possible to create situations in which the visual and auditory percepts do not match.</div><div>
<a href="MPC%20Day%204%3A%20Auditory%20scene%20analysis%20and%20auditory%20illusions.%202_4_14.html.resources/McGurk_large.mov">McGurk_large.mov</a> </div><div>
(video source: <a title="External link to http://www.media.uio.no/personer/arntm/McGurk_english.html" href="http://www.media.uio.no/personer/arntm/McGurk_english.html">http://www.media.uio.no/personer/arntm/McGurk_english.html</a>)</div><div>
A demonstration of the McGurk Effect. To experience the McGurk Effect, play this video clip in three different ways:</div><ol><li><div>normally: both watching and listening to the video (fused condition)</div></li><li><div>with your eyes shut but while listening to the clip (auditory condition)</div></li><li><div>with your eyes open but with the sound turned off (visual condition)</div></li></ol><div>
The video clip consists of a video of the person saying "ga ga", dubbed with a voice saying "ba ba". Thus, when viewing the video with the sound turned off, you should see the person saying "ga ga", and when listening to the video with your eyes closed you should hjavascript:;</div><div>
doneear "ba ba". Interestingly, in the fused condition when both looking and listening, most people report hearing "da da", which is a fusion of "ba ba" and "ga ga".</div><div>
The McGurkEffect was first report in 1976 by McGurk and MacDonald, who propose the following explanation for their finding:</div><blockquote>... in a ga-lips/ba-voice representation, there is visual information for [a] and [da] and auditory information with features common to [ba] and [da]. By responding to the common information in both modalities, a subject would arrive at the unifying percept /da/. (McGurk &amp; MacDonald, <span style="font-style: italic;">Nature</span>, 1976).</blockquote></div><div>LinksnDownloads</div><div>PsycheLoui, 7 June 2006 (created 23 May 2006)</div><div><ul><li><div><a title="External link to http://hyperphysics.phy-astr.gsu.edu/Hbase/acoustic/auditcon.html#c1" href="http://hyperphysics.phy-astr.gsu.edu/Hbase/acoustic/auditcon.html#c1">Auditorium acoustics concepts</a></div></li><li><div><a title="External link to http://ist-socrates.berkeley.edu/~twickens/DPRIMEW.EXE" href="http://ist-socrates.berkeley.edu/%7Etwickens/DPRIMEW.EXE">Dprime calculator</a> by Tom Wickens - see SignalDetectionTheory</div></li><li><div><a title="External link to http://wise.cgu.edu/sdt/sdt.html" href="http://wise.cgu.edu/sdt/sdt.html">Clarement Graduate School's Signal Detection Theory Demo</a></div></li><li><div><a title="External link to http://cognet.mit.edu/library/books/view?isbn=0262521954" href="http://cognet.mit.edu/library/books/view?isbn=0262521954">Auditory Scene Analysis</a> - Bregman's book on MIT Press</div></li><li><div><a title="External link to http://caliber.ucpress.net/loi/mp" href="http://caliber.ucpress.net/loi/mp">Music Perception journal</a></div></li><li><div><br/></div></li></ul></div><div><br/></div></body></html>